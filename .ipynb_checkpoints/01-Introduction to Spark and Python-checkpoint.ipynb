{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark and Python\n",
    "\n",
    "Let's learn how to use Spark with Python by using the pyspark library! Make sure to view the video lecture explaining Spark and RDDs before continuing on with this code.\n",
    "\n",
    "This notebook will serve as reference code for the Big Data section of the course involving Amazon Web Services. The video will provide fuller explanations for what the code is doing.\n",
    "\n",
    "## Creating a SparkContext\n",
    "\n",
    "First we need to create a SparkContext. We will import this from pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_spark_session(\n",
    "    app_name, driver_cores, driver_mem, max_executors, executor_cores,\n",
    "    executor_mem, queue\n",
    "):\n",
    "    \"\"\"Build Spark session.\"\"\"\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .config(\"spark.master\", \"yarn\")\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\n",
    "        .config(\"spark.driver.cores\", driver_cores)\n",
    "        .config(\"spark.driver.memory\", driver_mem)\n",
    "        .config(\"spark.executor.cores\", executor_cores)\n",
    "        .config(\"spark.executor.memory\", executor_mem)\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", 0)\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", max_executors)\n",
    "        .config(\"spark.executor.memoryOverhead\", 2048)\n",
    "        .config(\"spark.driver.memoryOverhead\", 1024)\n",
    "        .config(\"spark.yarn.queue\", queue)\n",
    "        # .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "        .config(\"spark.driver.extraClassPath\", \"/soft/ora1210/db/jdbc/lib/ojdbc6.jar\")\n",
    "        .config(\"spark.executor.extraClassPath\", \"/soft/ora1210/db/jdbc/lib/ojdbc6.jar\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder\\\n",
    "        .appName(\"app_name\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster.\n",
    "\n",
    "*Note! You can only have one SparkContext at a time the way we are running things here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations\n",
    "\n",
    "We're going to start with a 'hello world' example, which is just reading a text file. First let's create a text file.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's write an example text file to read, we'll use some special jupyter notebook commands for this, but feel free to use any .txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take in the textfile using the **textFile** method off of the SparkContext we created. This method will read a text file from HDFS, a local file system (available on all\n",
    "nodes), or any Hadoop-supported file system URI, and return it as an RDD of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. \n",
    "\n",
    "### Actions\n",
    "\n",
    "We have just created an RDD using the textFile method and can perform operations on this object, such as counting the rows.\n",
    "\n",
    "RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Now we can use transformations, for example the filter transformation will return a new RDD with a subset of items in the file. Let's create a sample transformation using the filter() method. This method (just like Python's own filter function) will only return elements that satisfy the condition. Let's try looking for lines that contain the word 'second'. In which case, there should only be one line that has that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secfind = textFile.filter(lambda line: 'second' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD\n",
    "secfind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second line']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform action on transformation\n",
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform action on transformation\n",
    "secfind.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import datetime\n",
    "\n",
    "l = [(datetime.date(2018,1,3), 'Ankit',25, 'F'),\n",
    "     (datetime.date(2018,2,3), 'Jalfaizy',22, 'M'),\n",
    "     (datetime.date(2018,1,5), 'saurabh',20, 'F'),\n",
    "     (datetime.date(2018,1,12), 'Bala',26, 'F'),\n",
    "     (datetime.date(2018,7,9), 'Jules',19, 'M') ,\n",
    "     (datetime.date(2018,3,18), 'Arild',43, 'M'),\n",
    "     (datetime.date(2018,1,5), 'sarah',20, 'F'),\n",
    "     (datetime.date(2018,8,12), 'Boly',33, 'M'),\n",
    "     (datetime.date(2018,4,6), 'Anita',35, 'F'),\n",
    "     (datetime.date(2018,12,6), 'Jules',22, 'M'),\n",
    "     (datetime.date(2018,7,24), 'Soul',20, 'M'),\n",
    "     (datetime.date(2018,6,17), 'Gral',54, 'F'),\n",
    "     (datetime.date(2018,9,7), 'Apoh',18, 'M'),\n",
    "     (datetime.date(2018,10,4), 'Dony',32, 'M'),\n",
    "     (datetime.date(2018,2,5), 'Tanoh',31, 'M'),\n",
    "     (datetime.date(2018,11,12), 'Issouf',27, 'M'),\n",
    "     (datetime.date(2018,10,3), 'Bilé',29, 'F'),\n",
    "     (datetime.date(2018,5,3), 'Gagnon',20, 'M'),\n",
    "     (datetime.date(2018,3,5), 'Papiss',28, 'F'),\n",
    "     (datetime.date(2018,2,12), 'Kravitz',34, 'F'),\n",
    "     (datetime.date(2018,5,9), 'Mouli',35, 'F'),\n",
    "     (datetime.date(2018,8,3), 'Jacques',27, 'M'),\n",
    "     (datetime.date(2018,12,5), 'soum',22, 'M'),\n",
    "     (datetime.date(2018,4,12), 'MBra',36, 'F')]\n",
    "\n",
    "rdd = spark_session.sparkContext.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(date=x[0], name=x[1], age=int(x[2])))\n",
    "schemaPeople = spark_session.createDataFrame(people)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1- compter le nombre de personne totale 2- compter le nombre de fille et de garcon 3- quel est l'age moyen, median mini et maxi dans chaque groupe (garcon, fille) 4 - quelle est le nombre de jours écoulé depuis la dernière visite pour chaque client (la colonne date correspond à la date de visite) 5 - quels sont les personnes qui ont une date de visite < 400 jours. 6- verifier si il existe des valeurs manquantes dans les données 7- rajouter une colonne mois correspondant au mois de chaque individu dans le data frame 8- creer une colonne adulte qui prendra la valeur 0 pour les personnes de moins de 25ans et 1 dans le cas contraire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the transformations won't display an output and won't be run until an action is called. In the next lecture: Advanced Spark and Python we will begin to see many more examples of this transformation and action relationship!\n",
    "\n",
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
